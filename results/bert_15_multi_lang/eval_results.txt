alpha=0.5
bert_model=bert-base-multilingual-cased
cache_dir=./stancy/BERT_CACHE/
data_dir=/content/stancy/data/multi-lang
do_eval=True
do_lower_case=True
do_train=False
eval_batch_size=8
fp16=False
gradient_accumulation_steps=1
learning_rate=1e-05
local_rank=-1
loss_scale=0.0
max_seq_length=128
no_cuda=False
num_train_epochs=15
output_dir=/content/stancy/results/bert_15_multi_lang
seed=42
server_ip=
server_port=
task_name=multi_lang
train_batch_size=24
warmup_proportion=0.1
eval_accuracy = 0.5564223456062533
eval_loss = 1.8842690972104916
global_step = 0
loss = None
              precision    recall  f1-score   support

    supports     0.5437    0.7655    0.6358     32618
     refutes     0.5878    0.3423    0.4327     31860

    accuracy                         0.5564     64478
   macro avg     0.5658    0.5539    0.5343     64478
weighted avg     0.5655    0.5564    0.5355     64478
